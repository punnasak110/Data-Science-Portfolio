{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "869854fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ‡πÇ‡∏´‡∏•‡∏î dataset ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà: 14580 ‡∏£‡∏π‡∏õ\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:5153/?notebook=True&subscription=b11f3e18-d876-4875-93e7-280717105dcb\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x29950a29bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# -------------------------\n",
    "# 1. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ FiftyOne\n",
    "# -------------------------\n",
    "config = fo.config\n",
    "config.database_validation = False\n",
    "\n",
    "# -------------------------\n",
    "# 2. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏û‡∏≤‡∏ò\n",
    "# -------------------------\n",
    "dataset_dir = r\"C:\\Users\\HP\\Downloads\\ThaiCharacter Dataset\\round2\"\n",
    "dataset_name = \"CharRecognitionProject_New\"\n",
    "\n",
    "base_export_dir = r\"C:\\Users\\HP\\Downloads\\ThaiCharacter Dataset\\Separated_Images\"\n",
    "training_dir = r\"C:\\Users\\HP\\Downloads\\ThaiCharacter Dataset\\Final_Training_Set\"\n",
    "\n",
    "# -------------------------\n",
    "# 3. ‡πÇ‡∏´‡∏•‡∏î‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á dataset\n",
    "# -------------------------\n",
    "if dataset_name in fo.list_datasets():\n",
    "    dataset = fo.load_dataset(dataset_name)\n",
    "    print(f\"‚úÖ ‡πÇ‡∏´‡∏•‡∏î dataset ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà: {len(dataset)} ‡∏£‡∏π‡∏õ\")\n",
    "else:\n",
    "    dataset = fo.Dataset.from_dir(\n",
    "        dataset_dir=dataset_dir,\n",
    "        dataset_type=fo.types.ImageClassificationDirectoryTree,\n",
    "        name=dataset_name\n",
    "    )\n",
    "    print(f\"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á dataset ‡πÉ‡∏´‡∏°‡πà: {len(dataset)} ‡∏£‡∏π‡∏õ\")\n",
    "\n",
    "# -------------------------\n",
    "# 4. ‡∏™‡∏£‡πâ‡∏≤‡∏á field 'tags' ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ\n",
    "# -------------------------\n",
    "if \"tags\" not in dataset.get_field_schema():\n",
    "    dataset.add_sample_field(\"tags\", fo.types.List, embedded_type=fo.types.String)\n",
    "    print(\"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á field 'tags' ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß\")\n",
    "\n",
    "# -------------------------\n",
    "# 5. ‡πÄ‡∏õ‡∏¥‡∏î FiftyOne App\n",
    "# -------------------------\n",
    "session = fo.launch_app(dataset, port=5153)  # desktop=False ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4fb82af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏ó‡πá‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß!\n",
      "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÅ‡∏ó‡πá‡∏Å: 651 ‡∏£‡∏π‡∏õ\n"
     ]
    }
   ],
   "source": [
    "# üíæ Cell ‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ï‡∏¥‡∏î‡πÅ‡∏ó‡πá‡∏Å‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß\n",
    "dataset.persistent = True\n",
    "dataset.save()\n",
    "print(\"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏ó‡πá‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß!\")\n",
    "print(f\"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÅ‡∏ó‡πá‡∏Å: {len(dataset.match_tags('WTF')) + len(dataset.match_tags('hardtosee'))} ‡∏£‡∏π‡∏õ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "181663be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 410/410 [1.4s elapsed, 0s remaining, 282.6 samples/s]         \n",
      "‚úÖ ‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å 410 ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÅ‡∏ó‡πá‡∏Å 'WTF' ‡πÑ‡∏õ‡∏¢‡∏±‡∏á: C:\\Users\\HP\\Downloads\\ThaiCharacter Dataset\\Separated_Images\\WTF\n",
      " 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 241/241 [799.5ms elapsed, 0s remaining, 301.4 samples/s]      \n",
      "‚úÖ ‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å 241 ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÅ‡∏ó‡πá‡∏Å 'hardtosee' ‡πÑ‡∏õ‡∏¢‡∏±‡∏á: C:\\Users\\HP\\Downloads\\ThaiCharacter Dataset\\Separated_Images\\hardtosee\n",
      " 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13929/13929 [47.0s elapsed, 0s remaining, 266.4 samples/s]      \n",
      "‚ú® ‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏†‡∏≤‡∏û‡∏î‡∏µ 13929 ‡∏£‡∏π‡∏õ ‡πÑ‡∏õ‡∏¢‡∏±‡∏á: C:\\Users\\HP\\Downloads\\ThaiCharacter Dataset\\Final_Training_Set ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß!\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 6. ‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡πÅ‡∏ó‡πá‡∏Å 'WTF' ‡πÅ‡∏•‡∏∞ 'hardtosee'\n",
    "# -------------------------\n",
    "for tag_name in [\"WTF\", \"hardtosee\"]:\n",
    "    export_dir = os.path.join(base_export_dir, tag_name)\n",
    "    \n",
    "    # ‡∏•‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÄ‡∏Å‡πà‡∏≤‡∏ó‡∏¥‡πâ‡∏á‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà\n",
    "    if os.path.exists(export_dir):\n",
    "        shutil.rmtree(export_dir)\n",
    "        print(f\"üóëÔ∏è ‡∏•‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå '{tag_name}' ‡πÄ‡∏Å‡πà‡∏≤‡∏ó‡∏¥‡πâ‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢\")\n",
    "    \n",
    "    view = dataset.match_tags(tag_name)\n",
    "    num_samples = len(view)\n",
    "    if num_samples > 0:\n",
    "        view.export(\n",
    "            export_dir=export_dir,\n",
    "            dataset_type=fo.types.ImageClassificationDirectoryTree,\n",
    "            label_field=\"ground_truth\",\n",
    "            export_media=True\n",
    "        )\n",
    "        print(f\"‚úÖ ‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å {num_samples} ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÅ‡∏ó‡πá‡∏Å '{tag_name}' ‡πÑ‡∏õ‡∏¢‡∏±‡∏á: {export_dir}\")\n",
    "    else:\n",
    "        print(f\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÅ‡∏ó‡πá‡∏Å '{tag_name}'\")\n",
    "\n",
    "# -------------------------\n",
    "# 7. ‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏†‡∏≤‡∏û‡∏î‡∏µ (‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÅ‡∏ó‡πá‡∏Å)\n",
    "# -------------------------\n",
    "# ‡∏•‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå training ‡πÄ‡∏Å‡πà‡∏≤‡∏ó‡∏¥‡πâ‡∏á‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà\n",
    "if os.path.exists(training_dir):\n",
    "    shutil.rmtree(training_dir)\n",
    "    print(f\"üóëÔ∏è ‡∏•‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå training ‡πÄ‡∏Å‡πà‡∏≤‡∏ó‡∏¥‡πâ‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢\")\n",
    "\n",
    "problem_tags_condition = (\n",
    "    (fo.ViewField(\"tags\").contains(\"WTF\")) |\n",
    "    (fo.ViewField(\"tags\").contains(\"hardtosee\"))\n",
    ")\n",
    "good_samples_view = dataset.match(~problem_tags_condition)\n",
    "num_good_samples = len(good_samples_view)\n",
    "\n",
    "if num_good_samples > 0:\n",
    "    good_samples_view.export(\n",
    "        export_dir=training_dir,\n",
    "        dataset_type=fo.types.ImageClassificationDirectoryTree,\n",
    "        label_field=\"ground_truth\",\n",
    "        export_media=True\n",
    "    )\n",
    "    print(f\"‚ú® ‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏†‡∏≤‡∏û‡∏î‡∏µ {num_good_samples} ‡∏£‡∏π‡∏õ ‡πÑ‡∏õ‡∏¢‡∏±‡∏á: {training_dir} ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏†‡∏≤‡∏û‡∏î‡∏µ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f42c745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'C:\\Users\\HP\\Downloads\\ThaiCharacter Dataset\\Separated_Images\\WTF' already exists; export will be merged with existing files\n",
      " 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 410/410 [1.2s elapsed, 0s remaining, 356.4 samples/s]         \n",
      "‚úÖ ‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å 410 ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÅ‡∏ó‡πá‡∏Å 'WTF' ‡πÑ‡∏õ‡∏¢‡∏±‡∏á: C:\\Users\\HP\\Downloads\\ThaiCharacter Dataset\\Separated_Images\\WTF\n",
      "Directory 'C:\\Users\\HP\\Downloads\\ThaiCharacter Dataset\\Separated_Images\\hardtosee' already exists; export will be merged with existing files\n",
      " 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 241/241 [676.4ms elapsed, 0s remaining, 356.3 samples/s]      \n",
      "‚úÖ ‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å 241 ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÅ‡∏ó‡πá‡∏Å 'hardtosee' ‡πÑ‡∏õ‡∏¢‡∏±‡∏á: C:\\Users\\HP\\Downloads\\ThaiCharacter Dataset\\Separated_Images\\hardtosee\n",
      "Directory 'C:\\Users\\HP\\Downloads\\ThaiCharacter Dataset\\Final_Training_Set' already exists; export will be merged with existing files\n",
      " 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13929/13929 [38.0s elapsed, 0s remaining, 406.8 samples/s]      \n",
      "‚ú® ‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏†‡∏≤‡∏û‡∏î‡∏µ 13929 ‡∏£‡∏π‡∏õ ‡πÑ‡∏õ‡∏¢‡∏±‡∏á: C:\\Users\\HP\\Downloads\\ThaiCharacter Dataset\\Final_Training_Set ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß!\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 6. ‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡πÅ‡∏ó‡πá‡∏Å 'WTF' ‡πÅ‡∏•‡∏∞ 'hardtosee'\n",
    "# -------------------------\n",
    "for tag_name in [\"WTF\", \"hardtosee\"]:\n",
    "    export_dir = os.path.join(base_export_dir, tag_name)\n",
    "    view = dataset.match_tags(tag_name)\n",
    "    num_samples = len(view)\n",
    "    if num_samples > 0:\n",
    "        view.export(\n",
    "            export_dir=export_dir,\n",
    "            dataset_type=fo.types.ImageClassificationDirectoryTree,\n",
    "            label_field=\"ground_truth\",\n",
    "            export_media=True\n",
    "        )\n",
    "        print(f\"‚úÖ ‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å {num_samples} ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÅ‡∏ó‡πá‡∏Å '{tag_name}' ‡πÑ‡∏õ‡∏¢‡∏±‡∏á: {export_dir}\")\n",
    "    else:\n",
    "        print(f\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÅ‡∏ó‡πá‡∏Å '{tag_name}'\")\n",
    "\n",
    "# -------------------------\n",
    "# 7. ‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏†‡∏≤‡∏û‡∏î‡∏µ (‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÅ‡∏ó‡πá‡∏Å)\n",
    "# -------------------------\n",
    "problem_tags_condition = (\n",
    "    (fo.ViewField(\"tags\").contains(\"WTF\")) |\n",
    "    (fo.ViewField(\"tags\").contains(\"hardtosee\"))\n",
    ")\n",
    "good_samples_view = dataset.match(~problem_tags_condition)\n",
    "num_good_samples = len(good_samples_view)\n",
    "\n",
    "if num_good_samples > 0:\n",
    "    good_samples_view.export(\n",
    "        export_dir=training_dir,\n",
    "        dataset_type=fo.types.ImageClassificationDirectoryTree,\n",
    "        label_field=\"ground_truth\",\n",
    "        export_media=True\n",
    "    )\n",
    "    print(f\"‚ú® ‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏†‡∏≤‡∏û‡∏î‡∏µ {num_good_samples} ‡∏£‡∏π‡∏õ ‡πÑ‡∏õ‡∏¢‡∏±‡∏á: {training_dir} ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏†‡∏≤‡∏û‡∏î‡∏µ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Train\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
